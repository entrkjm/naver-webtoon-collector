"""
Cloud Functions ì§„ì…ì : ë„¤ì´ë²„ ì›¹íˆ° ì£¼ê°„ ì°¨íŠ¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

ì´ í•¨ìˆ˜ëŠ” HTTP íŠ¸ë¦¬ê±°ë¡œ ì‹¤í–‰ë˜ë©°, ì „ì²´ ELT íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
- Extract: ë„¤ì´ë²„ ì›¹íˆ° APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
- Load Raw: GCSì— JSON ì›ë³¸ ì €ì¥
- Transform: ë°ì´í„° íŒŒì‹± ë° ì •ê·œí™”
- Load Refined: BigQueryì— ì •ì œëœ ë°ì´í„° ì €ì¥
"""

import json
import logging
import os
from datetime import date
from typing import Optional

import functions_framework

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
import sys
from pathlib import Path

# Cloud Functionsì—ì„œëŠ” /workspaceê°€ ë£¨íŠ¸
# ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
if os.path.exists('/workspace'):
    project_root = Path('/workspace')
    sys.path.insert(0, str(project_root))
else:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©: functions/pipeline_functionì—ì„œ srcë¡œ ì ‘ê·¼
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))
    
    # src ë””ë ‰í† ë¦¬ë„ ê²½ë¡œì— ì¶”ê°€
    src_path = project_root / 'src'
    if src_path.exists():
        sys.path.insert(0, str(src_path))

from src.extract import extract_webtoon_chart, try_api_endpoints
from src.parse import parse_html_file
from src.parse_api import parse_api_response
from src.transform import transform_and_save, load_dim_webtoon
from src.extract_webtoon_detail import extract_webtoon_detail
from src.transform_webtoon_stats import transform_and_save_webtoon_stats
from src.upload_gcs import upload_chart_data_to_gcs, upload_webtoon_detail_to_gcs
from src.upload_bigquery import (
    upload_dim_webtoon,
    upload_fact_weekly_chart,
    upload_fact_webtoon_stats,
    get_bigquery_client,
)
from src.utils import setup_logging

# í™˜ê²½ ë³€ìˆ˜
GCS_BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'naver-webtoon-raw')
BIGQUERY_PROJECT_ID = os.getenv('BIGQUERY_PROJECT_ID', 'naver-webtoon-collector')
BIGQUERY_DATASET_ID = os.getenv('BIGQUERY_DATASET_ID', 'naver_webtoon')

# ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)


@functions_framework.http
def main(request):
    """
    Cloud Functions HTTP íŠ¸ë¦¬ê±° ì§„ì…ì 
    
    Args:
        request: Flask Request ê°ì²´
    
    Returns:
        HTTP ì‘ë‹µ (JSON)
    """
    try:
        # ìš”ì²­ ë³¸ë¬¸ íŒŒì‹±
        request_json = request.get_json(silent=True)
        if request_json is None:
            request_json = {}
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        chart_date_str = request_json.get('date')
        if chart_date_str:
            try:
                chart_date = date.fromisoformat(chart_date_str)
            except ValueError:
                logger.error(f"ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {chart_date_str}")
                return {'error': f'Invalid date format: {chart_date_str}'}, 400
        else:
            chart_date = date.today()
        
        sort_types = request_json.get('sort_types', ['popular', 'view'])
        limit = request_json.get('limit')  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
        delete_existing = request_json.get('delete_existing', False)  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìš”ì²­ ì‹œ)
        if delete_existing:
            logger.info(f"\n{'='*60}")
            logger.info(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì‹œì‘: date={chart_date}")
            logger.info(f"{'='*60}")
            
            try:
                from google.cloud import bigquery
                client = get_bigquery_client()
                
                # fact_weekly_chartì—ì„œ í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì‚­ì œ
                delete_query = f"""
                DELETE FROM `{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_ID}.fact_weekly_chart`
                WHERE chart_date = '{chart_date}'
                """
                query_job = client.query(delete_query)
                query_job.result()
                deleted_count = query_job.num_dml_affected_rows if hasattr(query_job, 'num_dml_affected_rows') else 0
                logger.info(f"âœ… fact_weekly_chartì—ì„œ {deleted_count}ê°œ ë ˆì½”ë“œ ì‚­ì œë¨")
                
                # GCSì—ì„œ í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì‚­ì œ
                from google.cloud import storage
                storage_client = storage.Client(project=BIGQUERY_PROJECT_ID)
                bucket = storage_client.bucket(GCS_BUCKET_NAME)
                
                # ë‚ ì§œë³„ ê²½ë¡œ ì‚­ì œ
                date_prefix = f"raw_html/{chart_date}/"
                blobs = bucket.list_blobs(prefix=date_prefix)
                deleted_blobs = 0
                for blob in blobs:
                    blob.delete()
                    deleted_blobs += 1
                
                if deleted_blobs > 0:
                    logger.info(f"âœ… GCSì—ì„œ {deleted_blobs}ê°œ íŒŒì¼ ì‚­ì œë¨")
                else:
                    logger.info("GCSì— í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                # ì‚­ì œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        
        logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘: date={chart_date}, sort_types={sort_types}")
        
        all_success = True
        
        # ê° ì •ë ¬ íƒ€ì…ë³„ë¡œ ìˆ˜ì§‘
        for sort_type in sort_types:
            sort_name = sort_type if sort_type else "default"
            logger.info(f"\n{'='*60}")
            logger.info(f"ì •ë ¬ íƒ€ì…: {sort_name}")
            logger.info(f"{'='*60}")
            
            try:
                # Step 1: Extract (APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘)
                logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘... (ì •ë ¬: {sort_name})")
                api_data = try_api_endpoints(sort_type=sort_type)
                
                if api_data is None:
                    logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ì •ë ¬: {sort_name})")
                    all_success = False
                    continue
                
                # Step 2: Load Raw (GCSì— JSON ì›ë³¸ ì €ì¥)
                logger.info("GCSì— ì›ë³¸ ë°ì´í„° ì €ì¥ ì¤‘...")
                # ì„ì‹œ íŒŒì¼ì— ì €ì¥ í›„ GCS ì—…ë¡œë“œ
                from tempfile import NamedTemporaryFile
                import json as json_module
                
                with NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as tmp_file:
                    json_module.dump(api_data, tmp_file, ensure_ascii=False, indent=2)
                    tmp_path = Path(tmp_file.name)
                
                try:
                    gcs_success = upload_chart_data_to_gcs(
                        chart_date,
                        sort_type=sort_type,
                        json_file_path=tmp_path,
                        dry_run=False
                    )
                    if not gcs_success:
                        logger.warning(f"GCS ì—…ë¡œë“œ ì‹¤íŒ¨ (ì •ë ¬: {sort_name}), ê³„ì† ì§„í–‰...")
                finally:
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    if tmp_path.exists():
                        tmp_path.unlink()
                
                # Step 3: Parse (ë°ì´í„° íŒŒì‹±)
                logger.info("ë°ì´í„° íŒŒì‹± ì‹œì‘...")
                parsed_data = parse_api_response(api_data)
                
                if len(parsed_data) == 0:
                    logger.error("íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    all_success = False
                    continue
                
                logger.info(f"íŒŒì‹± ì™„ë£Œ: {len(parsed_data)}ê°œ ì›¹íˆ° ë°ì´í„°")
                
                # Step 4: Transform & Load Refined (BigQueryì— ì§ì ‘ ì €ì¥)
                # transform_and_saveë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ íŒŒì¼ì— ì €ì¥ í›„ BigQuery ì—…ë¡œë“œ
                logger.info("ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì‹œì‘...")
                
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš© (Cloud Functionsì˜ /tmp ì‚¬ìš©)
                import tempfile
                temp_dir = Path(tempfile.gettempdir()) / 'webtoon_pipeline'
                temp_dir.mkdir(parents=True, exist_ok=True)
                
                # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë¡œì»¬ íŒŒì¼ ì €ì¥ ê²½ë¡œ)
                os.environ['DATA_DIR'] = str(temp_dir)
                
                # transform_and_save ì‹¤í–‰ (ë¡œì»¬ íŒŒì¼ì— ì €ì¥)
                success = transform_and_save(parsed_data, chart_date, sort_type=sort_type)
                
                if success:
                    # ì €ì¥ëœ JSONL íŒŒì¼ì„ BigQueryì— ì—…ë¡œë“œ
                    from src.utils import get_dim_webtoon_jsonl_path, get_chart_jsonl_path
                    
                    # dim_webtoon ì—…ë¡œë“œ
                    dim_jsonl_path = get_dim_webtoon_jsonl_path()
                    if dim_jsonl_path.exists():
                        logger.info(f"dim_webtoon.jsonl íŒŒì¼ ë°œê²¬, BigQuery ì—…ë¡œë“œ ì‹œì‘: {dim_jsonl_path}")
                        try:
                            upload_success = upload_dim_webtoon(jsonl_path=dim_jsonl_path, dry_run=False)
                            if upload_success:
                                logger.info("dim_webtoon BigQuery ì—…ë¡œë“œ ì„±ê³µ")
                            else:
                                logger.error("dim_webtoon BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                        except Exception as e:
                            logger.error(f"dim_webtoon BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            import traceback
                            traceback.print_exc()
                    else:
                        logger.warning(f"dim_webtoon.jsonl íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dim_jsonl_path}")
                    
                    # fact_weekly_chart ì—…ë¡œë“œ
                    fact_jsonl_path = get_chart_jsonl_path(chart_date, sort_type)
                    if fact_jsonl_path.exists():
                        logger.info(f"fact_weekly_chart.jsonl íŒŒì¼ ë°œê²¬, BigQuery ì—…ë¡œë“œ ì‹œì‘: {fact_jsonl_path}")
                        try:
                            upload_success = upload_fact_weekly_chart(
                                chart_date=chart_date,
                                sort_type=sort_type,
                                jsonl_path=fact_jsonl_path,
                                dry_run=False
                            )
                            if upload_success:
                                logger.info("fact_weekly_chart BigQuery ì—…ë¡œë“œ ì„±ê³µ")
                            else:
                                logger.error("fact_weekly_chart BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                        except Exception as e:
                            logger.error(f"fact_weekly_chart BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            import traceback
                            traceback.print_exc()
                    else:
                        logger.warning(f"fact_weekly_chart.jsonl íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {fact_jsonl_path}")
                else:
                    logger.error(f"ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì‹¤íŒ¨ (ì •ë ¬: {sort_name})")
                    all_success = False
                    continue
                
                logger.info(f"âœ… ì •ë ¬ íƒ€ì… '{sort_name}' ìˆ˜ì§‘ ì™„ë£Œ!")
                
            except Exception as e:
                logger.error(f"ì •ë ¬ íƒ€ì… '{sort_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                all_success = False
        
        # ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (genre, tags ì •ë³´ ìˆ˜ì§‘) - í•„ìˆ˜ ì‹¤í–‰
        logger.info("\n" + "="*60)
        logger.info("ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘... (genre, tags ìˆ˜ì§‘)")
        logger.info("="*60)
        
        try:
            # dim_webtoonì—ì„œ ëª¨ë“  ì›¹íˆ° ID ê°€ì ¸ì˜¤ê¸°
            from src.transform import load_dim_webtoon
            dim_df = load_dim_webtoon()
            
            if len(dim_df) == 0:
                logger.warning("ìˆ˜ì§‘í•  ì›¹íˆ°ì´ ì—†ìŠµë‹ˆë‹¤. ì°¨íŠ¸ ìˆ˜ì§‘ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            else:
                webtoon_ids = dim_df['webtoon_id'].astype(str).unique().tolist()
                
                # limit íŒŒë¼ë¯¸í„° ì ìš© (í…ŒìŠ¤íŠ¸ìš©)
                if limit is not None and limit > 0:
                    webtoon_ids = webtoon_ids[:limit]
                    logger.info(f"ì œí•œ ëª¨ë“œ: {limit}ê°œ ì›¹íˆ°ë§Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
                
                logger.info(f"ì´ {len(webtoon_ids)}ê°œ ì›¹íˆ°ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
                logger.info(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {len(webtoon_ids) * 2 / 60:.1f}ë¶„ (ê° ì›¹íˆ°ë‹¹ ì•½ 2ì´ˆ)")
                
                detail_data_list = []
                batch_size = 10  # Rate limiting ë°°ì¹˜ í¬ê¸°
                batch_delay = 5  # Rate limiting ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
                save_batch_size = 100  # ì €ì¥ ë°°ì¹˜ í¬ê¸° (100ê°œë§ˆë‹¤ ì €ì¥ ë° ì—…ë¡œë“œ)
                
                from src.transform import merge_dim_webtoon, save_dim_webtoon, load_dim_webtoon
                from src.models import create_dim_webtoon_record
                
                # dim_webtoon ë¡œë“œ (ë°°ì¹˜ ì €ì¥ ì‹œ ì‚¬ìš©)
                dim_df = load_dim_webtoon()
                dim_df['webtoon_id'] = dim_df['webtoon_id'].astype(str)
                dim_webtoon_ids = set(dim_df['webtoon_id'])
                
                for i, webtoon_id in enumerate(webtoon_ids, 1):
                    try:
                        # ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                        detail_data = extract_webtoon_detail(webtoon_id, use_html_fallback=True)
                        
                        if detail_data:
                            detail_data_list.append(detail_data)
                            if i % 10 == 0:
                                logger.info(f"[{i}/{len(webtoon_ids)}] ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì§„í–‰ ì¤‘... (ì„±ê³µ: {len(detail_data_list)}ê°œ)")
                        else:
                            logger.warning(f"[{i}/{len(webtoon_ids)}] ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {webtoon_id}")
                        
                        # Rate limiting: ê° ìš”ì²­ ê°„ 1.5ì´ˆ ëŒ€ê¸°
                        import time
                        time.sleep(1.5)
                        
                        # Rate limiting ë°°ì¹˜ ì²˜ë¦¬: 10ê°œë§ˆë‹¤ ê¸´ ëŒ€ê¸°
                        if i % batch_size == 0 and i < len(webtoon_ids):
                            logger.info(f"Rate limiting ë°°ì¹˜ ì™„ë£Œ: {i}/{len(webtoon_ids)}ê°œ ì²˜ë¦¬ë¨. {batch_delay}ì´ˆ ëŒ€ê¸°...")
                            time.sleep(batch_delay)
                        
                        # ì €ì¥ ë°°ì¹˜ ì²˜ë¦¬: 100ê°œë§ˆë‹¤ ì €ì¥ ë° BigQuery ì—…ë¡œë“œ
                        if len(detail_data_list) >= save_batch_size and i % save_batch_size == 0:
                            logger.info(f"\n{'='*60}")
                            logger.info(f"ë°°ì¹˜ ì €ì¥ ì‹œì‘: {len(detail_data_list)}ê°œ ë°ì´í„° ì €ì¥ ë° ì—…ë¡œë“œ")
                            logger.info(f"{'='*60}")
                            
                            # í˜„ì¬ ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
                            batch_data = detail_data_list[:save_batch_size]
                            detail_data_list = detail_data_list[save_batch_size:]  # ë‚¨ì€ ë°ì´í„°ëŠ” ë‹¤ìŒ ë°°ì¹˜ì—ì„œ ì²˜ë¦¬
                            
                            # fact_webtoon_stats ì €ì¥
                            try:
                                stats_success = transform_and_save_webtoon_stats(batch_data, dim_webtoon_ids)
                                if stats_success:
                                    logger.info(f"âœ… fact_webtoon_stats ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(batch_data)}ê°œ")
                                    
                                    # fact_webtoon_statsë¥¼ BigQueryì— ì—…ë¡œë“œ
                                    from src.utils import get_webtoon_stats_jsonl_path
                                    stats_jsonl_path = get_webtoon_stats_jsonl_path()
                                    if stats_jsonl_path.exists():
                                        logger.info("fact_webtoon_statsë¥¼ BigQueryì— ì—…ë¡œë“œ ì¤‘...")
                                        try:
                                            upload_success = upload_fact_webtoon_stats(jsonl_path=stats_jsonl_path, dry_run=False)
                                            if upload_success:
                                                logger.info(f"âœ… fact_webtoon_stats BigQuery ì—…ë¡œë“œ ì„±ê³µ ({len(batch_data)}ê°œ)")
                                            else:
                                                logger.error("fact_webtoon_stats BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                                        except Exception as e:
                                            logger.error(f"fact_webtoon_stats BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                            import traceback
                                            traceback.print_exc()
                                else:
                                    logger.error("fact_webtoon_stats ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨")
                            except Exception as e:
                                logger.error(f"fact_webtoon_stats ë°°ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                import traceback
                                traceback.print_exc()
                            
                            # dim_webtoon ì—…ë°ì´íŠ¸ (genre, tags ì •ë³´ ì¶”ê°€)
                            logger.info("dim_webtoon ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘ (genre, tags ì •ë³´ ì¶”ê°€)...")
                            update_records = []
                            
                            for detail_data in batch_data:
                                webtoon_id = str(detail_data.get('webtoon_id')) if detail_data.get('webtoon_id') else None
                                genre = detail_data.get('genre')
                                tags = detail_data.get('tags')
                                
                                if webtoon_id and webtoon_id in dim_webtoon_ids:
                                    # ê¸°ì¡´ ë ˆì½”ë“œ ì°¾ê¸°
                                    existing = dim_df[dim_df['webtoon_id'] == webtoon_id]
                                    if len(existing) > 0:
                                        # genreë‚˜ tagsê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                                        if genre or tags:
                                            existing_record = existing.iloc[0].to_dict()
                                            
                                            # ê¸°ì¡´ tags ì²˜ë¦¬
                                            existing_tags = existing_record.get('tags')
                                            if isinstance(existing_tags, str):
                                                existing_tags = [t.strip() for t in existing_tags.split('|') if t.strip()] if existing_tags else []
                                            elif not isinstance(existing_tags, list):
                                                existing_tags = []
                                            
                                            # ìƒˆ tagsì™€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                                            new_tags = tags if tags else []
                                            if isinstance(new_tags, list):
                                                combined_tags = list(set(existing_tags + new_tags))
                                            else:
                                                combined_tags = existing_tags
                                            
                                            # ì—…ë°ì´íŠ¸ ë ˆì½”ë“œ ìƒì„±
                                            update_record = create_dim_webtoon_record(
                                                webtoon_id=webtoon_id,
                                                title=existing_record.get('title', ''),
                                                author=existing_record.get('author'),
                                                genre=genre if genre else existing_record.get('genre'),
                                                tags=combined_tags if combined_tags else None,
                                            )
                                            update_records.append(update_record)
                            
                            if len(update_records) > 0:
                                dim_df = merge_dim_webtoon(dim_df, update_records)
                                save_dim_webtoon(dim_df)
                                logger.info(f"âœ… dim_webtoon ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(update_records)}ê°œ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ë¨")
                                
                                # ì—…ë°ì´íŠ¸ëœ dim_webtoonì„ BigQueryì— ì—…ë¡œë“œ
                                from src.utils import get_dim_webtoon_jsonl_path
                                dim_jsonl_path = get_dim_webtoon_jsonl_path()
                                if dim_jsonl_path.exists():
                                    logger.info("ì—…ë°ì´íŠ¸ëœ dim_webtoonì„ BigQueryì— ì—…ë¡œë“œ ì¤‘...")
                                    try:
                                        upload_success = upload_dim_webtoon(jsonl_path=dim_jsonl_path, dry_run=False)
                                        if upload_success:
                                            logger.info(f"âœ… dim_webtoon BigQuery ì—…ë¡œë“œ ì„±ê³µ ({len(update_records)}ê°œ ì—…ë°ì´íŠ¸)")
                                        else:
                                            logger.error("dim_webtoon BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                                    except Exception as e:
                                        logger.error(f"dim_webtoon BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                        import traceback
                                        traceback.print_exc()
                            
                            logger.info(f"âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {i}/{len(webtoon_ids)}ê°œ ì²˜ë¦¬ë¨")
                            logger.info(f"{'='*60}\n")
                            
                    except Exception as e:
                        logger.error(f"ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ (webtoon_id={webtoon_id}): {e}")
                        continue
                
                # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬ (ë§ˆì§€ë§‰ ë°°ì¹˜)
                if len(detail_data_list) > 0:
                    logger.info(f"\n{'='*60}")
                    logger.info(f"ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì‹œì‘: {len(detail_data_list)}ê°œ ë°ì´í„° ì €ì¥ ë° ì—…ë¡œë“œ")
                    logger.info(f"{'='*60}")
                    
                    # fact_webtoon_stats ì €ì¥
                    try:
                        stats_success = transform_and_save_webtoon_stats(detail_data_list, dim_webtoon_ids)
                        if stats_success:
                            logger.info(f"âœ… fact_webtoon_stats ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {len(detail_data_list)}ê°œ")
                            
                            # fact_webtoon_statsë¥¼ BigQueryì— ì—…ë¡œë“œ
                            from src.utils import get_webtoon_stats_jsonl_path
                            stats_jsonl_path = get_webtoon_stats_jsonl_path()
                            if stats_jsonl_path.exists():
                                logger.info("fact_webtoon_statsë¥¼ BigQueryì— ì—…ë¡œë“œ ì¤‘...")
                                try:
                                    upload_success = upload_fact_webtoon_stats(jsonl_path=stats_jsonl_path, dry_run=False)
                                    if upload_success:
                                        logger.info(f"âœ… fact_webtoon_stats BigQuery ì—…ë¡œë“œ ì„±ê³µ ({len(detail_data_list)}ê°œ)")
                                    else:
                                        logger.error("fact_webtoon_stats BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                                except Exception as e:
                                    logger.error(f"fact_webtoon_stats BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                    import traceback
                                    traceback.print_exc()
                        else:
                            logger.error("fact_webtoon_stats ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨")
                    except Exception as e:
                        logger.error(f"fact_webtoon_stats ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        import traceback
                        traceback.print_exc()
                    
                    # dim_webtoon ì—…ë°ì´íŠ¸ (genre, tags ì •ë³´ ì¶”ê°€)
                    logger.info("dim_webtoon ë§ˆì§€ë§‰ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘ (genre, tags ì •ë³´ ì¶”ê°€)...")
                    update_records = []
                    
                    for detail_data in detail_data_list:
                        webtoon_id = str(detail_data.get('webtoon_id')) if detail_data.get('webtoon_id') else None
                        genre = detail_data.get('genre')
                        tags = detail_data.get('tags')
                        
                        if webtoon_id and webtoon_id in dim_webtoon_ids:
                            # ê¸°ì¡´ ë ˆì½”ë“œ ì°¾ê¸°
                            existing = dim_df[dim_df['webtoon_id'] == webtoon_id]
                            if len(existing) > 0:
                                # genreë‚˜ tagsê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                                if genre or tags:
                                    existing_record = existing.iloc[0].to_dict()
                                    
                                    # ê¸°ì¡´ tags ì²˜ë¦¬
                                    existing_tags = existing_record.get('tags')
                                    if isinstance(existing_tags, str):
                                        existing_tags = [t.strip() for t in existing_tags.split('|') if t.strip()] if existing_tags else []
                                    elif not isinstance(existing_tags, list):
                                        existing_tags = []
                                    
                                    # ìƒˆ tagsì™€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
                                    new_tags = tags if tags else []
                                    if isinstance(new_tags, list):
                                        combined_tags = list(set(existing_tags + new_tags))
                                    else:
                                        combined_tags = existing_tags
                                    
                                    # ì—…ë°ì´íŠ¸ ë ˆì½”ë“œ ìƒì„±
                                    update_record = create_dim_webtoon_record(
                                        webtoon_id=webtoon_id,
                                        title=existing_record.get('title', ''),
                                        author=existing_record.get('author'),
                                        genre=genre if genre else existing_record.get('genre'),
                                        tags=combined_tags if combined_tags else None,
                                    )
                                    update_records.append(update_record)
                    
                    if len(update_records) > 0:
                        dim_df = merge_dim_webtoon(dim_df, update_records)
                        save_dim_webtoon(dim_df)
                        logger.info(f"âœ… dim_webtoon ë§ˆì§€ë§‰ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(update_records)}ê°œ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ë¨")
                        
                        # ì—…ë°ì´íŠ¸ëœ dim_webtoonì„ BigQueryì— ì—…ë¡œë“œ
                        from src.utils import get_dim_webtoon_jsonl_path
                        dim_jsonl_path = get_dim_webtoon_jsonl_path()
                        if dim_jsonl_path.exists():
                            logger.info("ì—…ë°ì´íŠ¸ëœ dim_webtoonì„ BigQueryì— ì—…ë¡œë“œ ì¤‘...")
                            try:
                                upload_success = upload_dim_webtoon(jsonl_path=dim_jsonl_path, dry_run=False)
                                if upload_success:
                                    logger.info(f"âœ… dim_webtoon BigQuery ì—…ë¡œë“œ ì„±ê³µ ({len(update_records)}ê°œ ì—…ë°ì´íŠ¸)")
                                else:
                                    logger.error("dim_webtoon BigQuery ì—…ë¡œë“œ ì‹¤íŒ¨")
                            except Exception as e:
                                logger.error(f"dim_webtoon BigQuery ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                import traceback
                                traceback.print_exc()
                    else:
                        logger.warning(f"dim_webtoon ì—…ë°ì´íŠ¸í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. (genre/tagsê°€ ìˆëŠ” detail_data: {sum(1 for d in detail_data_list if d.get('genre') or d.get('tags'))}ê°œ)")
                    
                    logger.info(f"âœ… ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ")
                    logger.info(f"{'='*60}\n")
                else:
                    logger.warning("ìˆ˜ì§‘ëœ ì›¹íˆ° ìƒì„¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ì›¹íˆ° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            all_success = False
        
        if all_success:
            logger.info("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            return {'status': 'success', 'date': str(chart_date)}, 200
        else:
            logger.error("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ")
            return {'status': 'partial_failure', 'date': str(chart_date)}, 500
            
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {'error': str(e)}, 500

