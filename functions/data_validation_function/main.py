"""
ë°ì´í„° ê²€ì¦ Cloud Function
ì£¼ê¸°ì ìœ¼ë¡œ BigQuery ë°ì´í„°ë¥¼ í™•ì¸í•˜ì—¬ ìˆ˜ì§‘ ì‹¤íŒ¨ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
"""

import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, Any

from google.cloud import bigquery
from google.cloud import monitoring_v3

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# í™˜ê²½ ë³€ìˆ˜
BIGQUERY_PROJECT_ID = os.environ.get("BIGQUERY_PROJECT_ID", "naver-webtoon-collector")
BIGQUERY_DATASET_ID = os.environ.get("BIGQUERY_DATASET_ID", "naver_webtoon")
MIN_EXPECTED_RECORDS = int(os.environ.get("MIN_EXPECTED_RECORDS", "500"))  # ìµœì†Œ ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 500)
NOTIFICATION_CHANNEL_EMAIL = os.environ.get("NOTIFICATION_CHANNEL_EMAIL", "")


def get_bigquery_client():
    """BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return bigquery.Client(project=BIGQUERY_PROJECT_ID)


def check_data_collection(date_str: str = None) -> Dict[str, Any]:
    """
    ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        date_str: í™•ì¸í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹, Noneì´ë©´ ì˜¤ëŠ˜)
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if date_str is None:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    client = get_bigquery_client()
    
    results = {
        "date": date_str,
        "timestamp": datetime.now().isoformat(),
        "checks": {},
        "all_passed": True,
        "errors": []
    }
    
    try:
        # 1. fact_weekly_chart í™•ì¸
        chart_query = f"""
        SELECT 
            COUNT(*) AS total_records,
            COUNT(DISTINCT webtoon_id) AS unique_webtoons
        FROM `{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_ID}.fact_weekly_chart`
        WHERE chart_date = '{date_str}'
        """
        
        chart_job = client.query(chart_query)
        chart_results = list(chart_job.result())
        
        if chart_results:
            chart_data = chart_results[0]
            chart_count = chart_data.total_records
            chart_unique = chart_data.unique_webtoons
            
            results["checks"]["fact_weekly_chart"] = {
                "total_records": chart_count,
                "unique_webtoons": chart_unique,
                "passed": chart_count >= MIN_EXPECTED_RECORDS
            }
            
            if chart_count < MIN_EXPECTED_RECORDS:
                results["all_passed"] = False
                results["errors"].append(
                    f"fact_weekly_chart: ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜({MIN_EXPECTED_RECORDS})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. "
                    f"ì‹¤ì œ: {chart_count}ê°œ"
                )
        else:
            results["all_passed"] = False
            results["checks"]["fact_weekly_chart"] = {
                "total_records": 0,
                "unique_webtoons": 0,
                "passed": False
            }
            results["errors"].append(f"fact_weekly_chart: {date_str} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. fact_webtoon_stats í™•ì¸
        stats_query = f"""
        SELECT 
            COUNT(*) AS total_records,
            COUNT(DISTINCT webtoon_id) AS unique_webtoons,
            MAX(collected_at) AS last_collected
        FROM `{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_ID}.fact_webtoon_stats`
        WHERE DATE(collected_at) = '{date_str}'
        """
        
        stats_job = client.query(stats_query)
        stats_results = list(stats_job.result())
        
        if stats_results:
            stats_data = stats_results[0]
            stats_count = stats_data.total_records
            stats_unique = stats_data.unique_webtoons
            last_collected = stats_data.last_collected
            
            results["checks"]["fact_webtoon_stats"] = {
                "total_records": stats_count,
                "unique_webtoons": stats_unique,
                "last_collected": str(last_collected) if last_collected else None,
                "passed": stats_count >= MIN_EXPECTED_RECORDS
            }
            
            if stats_count < MIN_EXPECTED_RECORDS:
                results["all_passed"] = False
                results["errors"].append(
                    f"fact_webtoon_stats: ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜({MIN_EXPECTED_RECORDS})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. "
                    f"ì‹¤ì œ: {stats_count}ê°œ"
                )
        else:
            results["all_passed"] = False
            results["checks"]["fact_webtoon_stats"] = {
                "total_records": 0,
                "unique_webtoons": 0,
                "last_collected": None,
                "passed": False
            }
            results["errors"].append(f"fact_webtoon_stats: {date_str} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. ìµœê·¼ ìˆ˜ì§‘ ì‹œê°„ í™•ì¸ (24ì‹œê°„ ì´ë‚´ì— ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€)
        recent_query = f"""
        SELECT 
            MAX(collected_at) AS last_collected
        FROM `{BIGQUERY_PROJECT_ID}.{BIGQUERY_DATASET_ID}.fact_webtoon_stats`
        WHERE DATE(collected_at) >= DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY)
        """
        
        recent_job = client.query(recent_query)
        recent_results = list(recent_job.result())
        
        if recent_results and recent_results[0].last_collected:
            last_collected = recent_results[0].last_collected
            if isinstance(last_collected, str):
                last_collected = datetime.fromisoformat(last_collected.replace('Z', '+00:00'))
            
            hours_ago = (datetime.now(last_collected.tzinfo) - last_collected).total_seconds() / 3600
            
            results["checks"]["recent_collection"] = {
                "last_collected": str(last_collected),
                "hours_ago": round(hours_ago, 2),
                "passed": hours_ago < 48  # 48ì‹œê°„ ì´ë‚´ì— ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€
            }
            
            if hours_ago >= 48:
                results["all_passed"] = False
                results["errors"].append(
                    f"ìµœê·¼ ìˆ˜ì§‘ ì‹œê°„: {round(hours_ago, 2)}ì‹œê°„ ì „ "
                    f"(ë§ˆì§€ë§‰ ìˆ˜ì§‘: {last_collected})"
                )
        else:
            results["all_passed"] = False
            results["checks"]["recent_collection"] = {
                "last_collected": None,
                "hours_ago": None,
                "passed": False
            }
            results["errors"].append("ìµœê·¼ 2ì¼ ì´ë‚´ ë°ì´í„° ìˆ˜ì§‘ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        results["all_passed"] = False
        results["errors"].append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    return results


def send_alert(message: str, subject: str = "íŒŒì´í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì•Œë¦¼"):
    """
    ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        message: ì•Œë¦¼ ë©”ì‹œì§€
        subject: ì•Œë¦¼ ì œëª©
    """
    if not NOTIFICATION_CHANNEL_EMAIL:
        logger.warning("NOTIFICATION_CHANNEL_EMAILì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì•Œë¦¼ì„ ì „ì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # Cloud Monitoring APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œë¦¼ ì „ì†¡
        # ë˜ëŠ” ê°„ë‹¨í•˜ê²Œ ë¡œê·¸ì— ê¸°ë¡ (Cloud Loggingì´ ìë™ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡)
        logger.error(f"ğŸš¨ {subject}: {message}")
        
        # TODO: ì‹¤ì œ ì´ë©”ì¼ ì „ì†¡ ë¡œì§ êµ¬í˜„ (SendGrid, Mailgun ë“± ì‚¬ìš©)
        # í˜„ì¬ëŠ” Cloud Loggingì„ í†µí•´ ì•Œë¦¼ì´ ì „ì†¡ë˜ë„ë¡ í•¨
        
    except Exception as e:
        logger.error(f"ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}", exc_info=True)


def main(request):
    """
    Cloud Function ì§„ì…ì 
    
    Args:
        request: HTTP ìš”ì²­ ê°ì²´
    """
    try:
        # ìš”ì²­ì—ì„œ ë‚ ì§œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì„ íƒì‚¬í•­)
        if request.method == "POST":
            request_json = request.get_json(silent=True) or {}
            date_str = request_json.get("date")
        else:
            date_str = request.args.get("date")
        
        logger.info(f"ë°ì´í„° ê²€ì¦ ì‹œì‘: date={date_str}")
        
        # ë°ì´í„° ê²€ì¦ ì‹¤í–‰
        results = check_data_collection(date_str)
        
        # ê²°ê³¼ ë¡œê¹…
        logger.info(f"ê²€ì¦ ê²°ê³¼: all_passed={results['all_passed']}, errors={len(results['errors'])}ê°œ")
        
        # ì‹¤íŒ¨í•œ ê²½ìš° ì•Œë¦¼ ì „ì†¡
        if not results["all_passed"]:
            error_message = "\n".join(results["errors"])
            send_alert(
                message=f"ë‚ ì§œ: {results['date']}\n\nì˜¤ë¥˜:\n{error_message}\n\nìƒì„¸:\n{json.dumps(results, indent=2, ensure_ascii=False)}",
                subject=f"íŒŒì´í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ - {results['date']}"
            )
            
            return {
                "status": "failed",
                "message": "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ê°ì§€",
                "results": results
            }, 500
        else:
            logger.info("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼")
            return {
                "status": "success",
                "message": "ë°ì´í„° ìˆ˜ì§‘ ì •ìƒ",
                "results": results
            }, 200
            
    except Exception as e:
        logger.error(f"ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        send_alert(
            message=f"ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            subject="íŒŒì´í”„ë¼ì¸ ë°ì´í„° ê²€ì¦ í•¨ìˆ˜ ì˜¤ë¥˜"
        )
        return {
            "error": str(e),
            "status": "error"
        }, 500

