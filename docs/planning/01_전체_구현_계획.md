# 네이버 웹툰 주간 차트 수집 파이프라인 - 전체 구현 계획

## 프로젝트 개요

네이버 웹툰 주간 차트 데이터를 장기적으로 추적하고 분석하기 위한 ELT 파이프라인 구축 프로젝트입니다.

### 핵심 목표
- 매주 네이버 웹툰 주간 차트 데이터 수집 및 저장
- HTML 원본 보존 (GCS) + 정제된 데이터 저장 (BigQuery) 이중 구조
- GCP Always Free 범위 내에서 운영
- 확장 가능한 데이터 모델 설계

## 구현 단계별 계획

### Phase 1: 로컬 개발 및 테스트 (현재 단계)
**목표**: 핵심 로직을 로컬에서 구현하고 검증

#### 1.1 로컬 환경 설정
- Python 가상환경 설정
- 필요한 라이브러리 설치 (requests, beautifulsoup4, pandas 등)
- 로컬 파일 시스템으로 GCS/BigQuery 대체 구현

#### 1.2 핵심 로직 구현
- **Extract 모듈**: 네이버 웹툰 페이지 HTML 수집
- **Parse 모듈**: HTML에서 차트 데이터 파싱 (순위, 제목, 웹툰 ID 등)
- **Transform 모듈**: 파싱된 데이터를 스키마에 맞게 변환
- **로컬 저장 모듈**: 
  - HTML 원본 → 로컬 디렉토리 저장 (GCS 대체)
  - 정제된 데이터 → CSV/JSON 저장 (BigQuery 대체)

#### 1.3 데이터 모델 설계
- `dim_webtoon` 테이블 스키마 정의
- `fact_weekly_chart` 테이블 스키마 정의
- 관계형 데이터 모델 검증

#### 1.4 로컬 테스트
- 단일 실행 테스트
- 멱등성 테스트 (중복 실행 시 데이터 중복 방지)
- 에러 핸들링 테스트

**산출물**:
- 로컬에서 동작하는 완전한 파이프라인 코드
- 테스트 결과 및 검증 리포트
- 데이터 모델 스키마 문서

---

### Phase 2: GCP 인프라 단계별 구축

#### 2.1 GCP 프로젝트 설정
- GCP 프로젝트 생성 및 설정
- 필요한 API 활성화 (Cloud Functions, Cloud Scheduler, BigQuery, GCS)
- 서비스 계정 생성 및 권한 설정
- 로컬 인증 설정 (gcloud CLI)

**검증 항목**:
- GCP 프로젝트 접근 가능 여부
- 필요한 API 활성화 확인

---

#### 2.2 BigQuery 데이터 모델 구축
**목표**: 데이터 저장소 스키마 생성

**작업 내용**:
- BigQuery 데이터셋 생성
- `dim_webtoon` 테이블 생성 (스키마 정의)
- `fact_weekly_chart` 테이블 생성 (파티션 설정: 수집 날짜 기준)
- 샘플 데이터로 스키마 검증

**검증 항목**:
- 테이블 생성 확인
- 파티션 설정 확인
- 샘플 데이터 INSERT/SELECT 테스트

**산출물**:
- BigQuery 스키마 정의 파일 (SQL)
- 테이블 생성 스크립트

---

#### 2.3 GCS 버킷 생성 및 테스트
**목표**: HTML 원본 저장소 구축

**작업 내용**:
- GCS 버킷 생성
- 버킷 권한 설정
- 로컬에서 GCS 업로드/다운로드 테스트
- 파일명 규칙 정의 (예: `raw_html/YYYY-MM-DD/webtoon_chart.html`)

**검증 항목**:
- 버킷 생성 및 접근 가능 여부
- 파일 업로드/다운로드 테스트
- 파일명 규칙 준수 확인

**산출물**:
- GCS 버킷 설정 문서
- 파일 업로드 테스트 스크립트

---

#### 2.4 Cloud Functions - Extract 함수 구현
**목표**: HTML 수집 및 GCS 저장 함수 배포

**작업 내용**:
- Phase 1에서 구현한 Extract 로직을 Cloud Functions로 포팅
- GCS 클라이언트 라이브러리 연동
- HTTP 트리거 또는 Pub/Sub 트리거 설정
- 환경 변수 설정 (버킷명 등)
- 로컬에서 함수 테스트 (Functions Framework 사용)

**검증 항목**:
- 함수 배포 성공
- 수동 트리거 시 HTML 수집 및 GCS 저장 확인
- 에러 핸들링 동작 확인

**산출물**:
- Cloud Functions 코드 (Extract)
- 배포 스크립트
- 테스트 결과

---

#### 2.5 Cloud Functions - Transform 함수 구현
**목표**: GCS의 HTML을 파싱하여 BigQuery에 적재하는 함수 배포

**작업 내용**:
- Phase 1에서 구현한 Parse/Transform 로직을 Cloud Functions로 포팅
- GCS에서 HTML 다운로드 로직 추가
- BigQuery 클라이언트 라이브러리 연동
- 멱등성 보장 로직 구현 (파티션 기반 중복 체크)
- 환경 변수 설정 (프로젝트 ID, 데이터셋명 등)

**검증 항목**:
- 함수 배포 성공
- GCS에서 HTML 읽기 확인
- BigQuery 적재 확인
- 중복 실행 시 데이터 중복 방지 확인

**산출물**:
- Cloud Functions 코드 (Transform)
- 배포 스크립트
- 테스트 결과

---

#### 2.6 Cloud Scheduler 설정
**목표**: 주 1회 자동 실행 스케줄 설정

**작업 내용**:
- Cloud Scheduler 작업 생성
- Extract 함수 호출 스케줄 설정 (예: 매주 월요일 오전 9시)
- Transform 함수 호출 스케줄 설정 (Extract 이후 약간의 지연)
- 또는 Pub/Sub를 통한 함수 체이닝 구성

**검증 항목**:
- 스케줄러 작업 생성 확인
- 수동 실행 테스트
- 스케줄 설정 확인

**산출물**:
- Cloud Scheduler 설정 문서
- 스케줄 설정 스크립트

---

#### 2.7 GitHub Actions CI/CD 구축
**목표**: 코드 배포 자동화

**작업 내용**:
- GitHub Actions 워크플로우 파일 생성
- GCP 인증 설정 (서비스 계정 키 또는 Workload Identity)
- Cloud Functions 배포 자동화
- BigQuery 스키마 변경 시 자동 적용 (선택사항)

**검증 항목**:
- GitHub에 푸시 시 자동 배포 확인
- 배포 로그 확인

**산출물**:
- `.github/workflows/deploy.yml`
- CI/CD 문서

---

### Phase 3: 모니터링 및 최적화

#### 3.1 로깅 및 모니터링
- Cloud Functions 로그 확인
- BigQuery 쿼리 로그 확인
- 에러 알림 설정 (선택사항)

#### 3.2 비용 모니터링
- GCP 사용량 대시보드 확인
- Always Free 범위 내 사용 확인

#### 3.3 데이터 검증
- 수집된 데이터 품질 확인
- 주기적으로 데이터 일관성 검증

---

## 기술 스택

### 개발 환경
- **언어**: Python 3.9+
- **패키지 관리**: pip, requirements.txt
- **로컬 테스트**: pytest (선택사항)

### GCP 서비스
- **Cloud Functions**: 서버리스 함수 실행
- **Cloud Scheduler**: 주기적 작업 스케줄링
- **Cloud Storage (GCS)**: HTML 원본 저장
- **BigQuery**: 정제된 데이터 저장 및 분석

### 배포 및 관리
- **GitHub**: 코드 버전 관리
- **GitHub Actions**: CI/CD 자동화
- **gcloud CLI**: GCP 리소스 관리

---

## 예상 파일 구조

```
naver_webtoon/
├── start.md                          # 프로젝트 개요
├── 01_전체_구현_계획.md              # 본 문서
├── 02_로컬_테스트_계획.md            # 로컬 테스트 상세 계획
├── src/
│   ├── extract.py                    # HTML 수집 로직
│   ├── parse.py                      # HTML 파싱 로직
│   ├── transform.py                  # 데이터 변환 로직
│   ├── models.py                     # 데이터 모델 정의
│   └── utils.py                      # 유틸리티 함수
├── functions/
│   ├── extract_function/             # Extract Cloud Function
│   │   ├── main.py
│   │   └── requirements.txt
│   └── transform_function/           # Transform Cloud Function
│       ├── main.py
│       └── requirements.txt
├── tests/
│   ├── test_extract.py
│   ├── test_parse.py
│   └── test_transform.py
├── scripts/
│   ├── setup_bigquery.sql            # BigQuery 스키마 생성
│   └── deploy.sh                     # 배포 스크립트
├── .github/
│   └── workflows/
│       └── deploy.yml                # CI/CD 워크플로우
├── requirements.txt                   # 로컬 개발 의존성
└── README.md                          # 프로젝트 문서
```

---

## 주요 고려사항

### 멱등성 보장
- 수집 날짜(Partition)를 기준으로 중복 체크
- 같은 날짜에 여러 번 실행되어도 데이터가 중복되지 않도록 처리

### 에러 핸들링
- 네트워크 오류 시 재시도 로직
- HTML 구조 변경 시 대응 방안
- BigQuery 적재 실패 시 롤백 또는 재시도

### 확장성
- 향후 추가 수집 항목(작가, 별점, 댓글 수 등)을 쉽게 추가할 수 있는 구조
- `dim_webtoon` 테이블에 새로운 컬럼 추가 시 기존 데이터와의 호환성 유지

### 비용 관리
- GCS: Always Free (5GB 저장, 5,000 Class A 작업/월)
- BigQuery: Always Free (10GB 저장, 1TB 쿼리/월)
- Cloud Functions: Always Free (200만 요청/월, 400,000GB-초/월)
- Cloud Scheduler: Always Free (3개 작업 무료)

---

## 다음 단계

1. **02_로컬_테스트_계획.md** 파일을 참고하여 Phase 1 시작
2. 로컬에서 핵심 로직 구현 및 테스트
3. 검증 완료 후 Phase 2로 진행

