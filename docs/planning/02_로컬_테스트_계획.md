# 로컬 테스트 계획

## 목적

GCP 환경에 배포하기 전에 핵심 로직을 로컬에서 구현하고 검증하여, 안정적인 파이프라인을 구축합니다.

## 테스트 범위

### 포함되는 항목
- ✅ 네이버 웹툰 페이지 HTML 수집
- ✅ HTML 파싱 (순위, 제목, 웹툰 ID 추출)
- ✅ 데이터 변환 및 정규화
- ✅ 로컬 파일 시스템 저장 (GCS 대체)
- ✅ 로컬 CSV/JSON 저장 (BigQuery 대체)
- ✅ 멱등성 검증 (중복 실행 방지)
- ✅ 에러 핸들링

### 제외되는 항목 (Phase 2에서 구현)
- ❌ GCP 서비스 연동 (GCS, BigQuery)
- ❌ Cloud Functions 배포
- ❌ Cloud Scheduler 설정
- ❌ GitHub Actions CI/CD

---

## 테스트 환경 설정

### 1. Python 환경
```bash
# Python 3.9 이상 필요
python --version

# 가상환경 생성
python -m venv venv
source venv/bin/activate  # macOS/Linux
# 또는
venv\Scripts\activate  # Windows

# 의존성 설치
pip install -r requirements.txt
```

### 2. 필요한 라이브러리 (예상)
- `requests`: HTTP 요청 (웹 페이지 수집)
- `beautifulsoup4`: HTML 파싱
- `lxml` 또는 `html.parser`: HTML 파서
- `pandas`: 데이터 처리 (선택사항)
- `python-dateutil`: 날짜 처리

### 3. 디렉토리 구조
```
naver_webtoon/
├── data/
│   ├── raw/                    # HTML 원본 저장 (GCS 대체)
│   │   └── YYYY-MM-DD/
│   │       └── webtoon_chart.html
│   └── processed/              # 정제된 데이터 저장 (BigQuery 대체)
│       ├── dim_webtoon.csv
│       └── fact_weekly_chart.csv
├── src/
│   ├── extract.py
│   ├── parse.py
│   ├── transform.py
│   ├── models.py
│   └── utils.py
└── tests/
    └── test_*.py
```

---

## 테스트 시나리오

### 시나리오 1: 기본 수집 및 저장 테스트

**목표**: 단일 실행으로 데이터 수집부터 저장까지 전체 플로우 검증

**단계**:
1. 네이버 웹툰 주간 차트 페이지 접근
2. HTML 수집
3. `data/raw/YYYY-MM-DD/webtoon_chart.html`에 저장
4. HTML 파싱하여 순위, 제목, 웹툰 ID 추출
5. 데이터 변환 및 정규화
6. `data/processed/`에 CSV/JSON 저장

**검증 항목**:
- ✅ HTML 파일이 올바른 경로에 저장되었는지
- ✅ 파싱된 데이터가 예상 형식인지 (순위, 제목, ID 등)
- ✅ CSV/JSON 파일이 생성되었는지
- ✅ 데이터 개수가 예상 범위 내인지 (예: 10~100개)

**예상 결과**:
```
data/
├── raw/
│   └── 2024-01-15/
│       └── webtoon_chart.html  (수집된 HTML)
└── processed/
    ├── dim_webtoon.csv         (웹툰 마스터 데이터)
    └── fact_weekly_chart.csv   (주간 차트 히스토리)
```

---

### 시나리오 2: 멱등성 테스트

**목표**: 같은 날짜에 여러 번 실행해도 데이터가 중복되지 않는지 확인

**단계**:
1. 첫 번째 실행: 2024-01-15 데이터 수집 및 저장
2. 두 번째 실행: 같은 날짜(2024-01-15)로 다시 실행
3. 세 번째 실행: 다른 날짜(2024-01-16)로 실행

**검증 항목**:
- ✅ 같은 날짜로 재실행 시 `fact_weekly_chart`에 중복 레코드가 없는지
- ✅ `dim_webtoon`에는 새로운 웹툰만 추가되는지 (기존 웹툰은 업데이트 또는 유지)
- ✅ 다른 날짜로 실행 시 새로운 레코드가 추가되는지

**예상 결과**:
```
# 첫 실행 후
fact_weekly_chart.csv: 50개 레코드 (2024-01-15)
dim_webtoon.csv: 50개 레코드

# 같은 날짜 재실행 후
fact_weekly_chart.csv: 50개 레코드 (변화 없음)
dim_webtoon.csv: 50개 레코드 (변화 없음)

# 다른 날짜 실행 후
fact_weekly_chart.csv: 100개 레코드 (2024-01-15: 50개 + 2024-01-16: 50개)
dim_webtoon.csv: 50~100개 (새로운 웹툰이 있을 수 있음)
```

---

### 시나리오 3: 에러 핸들링 테스트

**목표**: 예외 상황에서도 시스템이 안정적으로 동작하는지 확인

**테스트 케이스**:

#### 3.1 네트워크 오류
- 인터넷 연결 차단 후 실행
- **예상 동작**: 재시도 로직 또는 명확한 에러 메시지

#### 3.2 HTML 구조 변경
- 수집한 HTML의 구조가 예상과 다를 때
- **예상 동작**: 파싱 실패 시 로그 기록 및 부분 데이터라도 저장

#### 3.3 파일 시스템 오류
- 저장 디렉토리 권한 없음
- **예상 동작**: 명확한 에러 메시지 및 실패 로그

#### 3.4 빈 데이터
- 파싱 결과가 비어있을 때
- **예상 동작**: 빈 파일 생성 또는 스킵 로직

---

### 시나리오 4: 데이터 모델 검증

**목표**: 설계한 데이터 모델이 올바르게 구현되었는지 확인

**검증 항목**:

#### dim_webtoon 테이블
- ✅ 웹툰 ID가 고유한지 (Primary Key)
- ✅ 필수 필드(제목, ID)가 모두 있는지
- ✅ 같은 웹툰이 여러 번 나타나도 하나의 레코드만 존재하는지

#### fact_weekly_chart 테이블
- ✅ 수집 날짜가 올바른지
- ✅ 순위가 1 이상의 정수인지
- ✅ 웹툰 ID가 `dim_webtoon`에 존재하는지 (Foreign Key 관계)
- ✅ 같은 날짜 + 같은 웹툰 ID 조합이 중복되지 않는지

**예상 스키마**:
```python
# dim_webtoon
{
    'webtoon_id': str,      # Primary Key
    'title': str,
    'author': str,          # 향후 확장용 (현재는 None 가능)
    'genre': str,           # 향후 확장용
    'created_at': datetime,
    'updated_at': datetime
}

# fact_weekly_chart
{
    'chart_date': date,     # Partition Key
    'webtoon_id': str,      # Foreign Key -> dim_webtoon
    'rank': int,
    'collected_at': datetime
}
```

---

## 구현 우선순위

### Phase 1-1: 기본 구조 및 Extract 모듈
1. 프로젝트 디렉토리 구조 생성
2. `src/extract.py`: 네이버 웹툰 페이지 HTML 수집
3. 로컬 파일 시스템에 HTML 저장
4. **테스트**: HTML 파일이 올바르게 저장되는지 확인

### Phase 1-2: Parse 모듈
1. `src/parse.py`: HTML에서 차트 데이터 파싱
2. BeautifulSoup을 사용한 데이터 추출
3. **테스트**: 파싱된 데이터가 예상 형식인지 확인

### Phase 1-3: Transform 모듈 및 데이터 모델
1. `src/models.py`: 데이터 모델 정의
2. `src/transform.py`: 파싱 데이터를 스키마에 맞게 변환
3. `dim_webtoon`과 `fact_weekly_chart` 분리 로직
4. **테스트**: CSV 파일이 올바른 스키마로 생성되는지 확인

### Phase 1-4: 멱등성 및 에러 핸들링
1. 중복 체크 로직 구현
2. 에러 핸들링 추가
3. 로깅 추가
4. **테스트**: 시나리오 2, 3 실행

### Phase 1-5: 통합 테스트
1. 전체 파이프라인 통합 테스트
2. 여러 날짜로 반복 실행 테스트
3. **최종 검증**: 모든 시나리오 통과 확인

---

## 검증 체크리스트

### 기능 검증
- [ ] HTML 수집 성공
- [ ] HTML 파일 로컬 저장 성공
- [ ] HTML 파싱 성공 (순위, 제목, ID 추출)
- [ ] 데이터 변환 성공
- [ ] CSV/JSON 파일 생성 성공
- [ ] 멱등성 보장 (중복 실행 시 데이터 중복 없음)
- [ ] 에러 핸들링 동작 확인

### 데이터 품질 검증
- [ ] `dim_webtoon`에 중복 웹툰 ID 없음
- [ ] `fact_weekly_chart`에 중복 (날짜, 웹툰 ID) 조합 없음
- [ ] 필수 필드 누락 없음
- [ ] 데이터 타입이 올바름 (순위는 정수, 날짜는 날짜 형식 등)
- [ ] Foreign Key 관계 유지 (`fact_weekly_chart.webtoon_id`가 `dim_webtoon.webtoon_id`에 존재)

### 코드 품질
- [ ] 함수 단위로 모듈화되어 있음
- [ ] 에러 핸들링이 적절함
- [ ] 로깅이 적절함
- [ ] 주석이 충분함
- [ ] 코드가 재사용 가능한 구조임

---

## 테스트 실행 방법

### 수동 테스트
```bash
# 1. 환경 설정
source venv/bin/activate
pip install -r requirements.txt

# 2. 기본 실행
python src/extract.py
python src/parse.py
python src/transform.py

# 또는 통합 스크립트
python run_pipeline.py --date 2024-01-15
```

### 자동화된 테스트 (선택사항)
```bash
# pytest 사용 시
pytest tests/ -v

# 특정 테스트만 실행
pytest tests/test_extract.py -v
```

---

## 예상 이슈 및 대응 방안

### 이슈 1: 네이버 웹툰 페이지 접근 제한
**가능성**: 높음 (봇 차단, User-Agent 체크 등)
**대응**: 
- User-Agent 헤더 추가
- 요청 간 딜레이 추가
- 세션 유지

### 이슈 2: HTML 구조 변경
**가능성**: 중간 (네이버가 페이지 구조 변경 시)
**대응**:
- 유연한 파싱 로직 (여러 선택자 시도)
- 파싱 실패 시 로그 기록 및 부분 데이터 저장

### 이슈 3: 데이터 중복
**가능성**: 낮음 (로직 구현 시 주의)
**대응**:
- 수집 날짜 + 웹툰 ID 조합으로 중복 체크
- INSERT 전 SELECT로 존재 여부 확인

---

## 다음 단계

로컬 테스트가 완료되면:
1. 테스트 결과 문서화
2. **01_전체_구현_계획.md**의 Phase 2로 진행
3. GCP 인프라 구축 시작

