[프로젝트 개요 및 설계 가이드라인]
1. 프로젝트 목적
네이버 웹툰 주간 차트 데이터의 장기적 추적: 매주 변하는 순위 데이터를 휘발시키지 않고 DB에 쌓아, 특정 작품의 흥행 추이나 장르별 인기도 변화를 SQL로 분석하기 위함.

데이터 자산화: 웹 페이지의 HTML 원본과 정제된 데이터를 모두 보존하여, 나중에 분석 항목이 늘어나더라도 과거 데이터를 재가공(Backfill)할 수 있는 구조 지향.

2. 핵심 설계 원칙 (ELT 아키텍처)
이 시스템은 일반적인 수집기보다 더 견고한 ELT(Extract-Load-Transform) 구조를 따릅니다.

Extract & Load Raw: 매주 한 번, 웹 페이지 전체 HTML을 수집하여 **GCS(Google Cloud Storage)**에 날짜별 파일로 저장. (데이터 원천 보존)

Transform & Load Refined: 저장된 HTML에서 필요한 정보(순위, 제목, ID 등)를 파싱하여 BigQuery의 관계형 테이블에 적재.

3. 주요 고려사항 (Decision Points)
A. 확장성 및 데이터 모델링
단순 리스트 저장이 아닌, **마스터 테이블(dim_webtoon)**과 **히스토리 테이블(fact_weekly_chart)**을 분리하는 스키마 설계.

나중에 작가 정보, 별점, 댓글 수 등 수집 항목이 늘어나도 기존 데이터와 쉽게 결합할 수 있는 구조.

B. 비용 및 효율성 (GCP Always Free 활용)
GCP의 무료 할당량을 초과하지 않도록 설계.

주 1회 실행이라는 특성에 맞춰 상주 서버(VM)를 띄우지 않고, 호출 시에만 작동하는 Cloud Functions와 Cloud Scheduler 조합을 선택.

C. 운영 및 코드 관리
GitHub을 코드 관리의 중심으로 사용.

GitHub Actions를 통해 GCP로의 배포(CI/CD)를 자동화하여, 로컬 환경 설정 없이도 클라우드 인프라를 형상 관리함.

멱등성(Idempotency) 보장: 중복 수집 시에도 데이터가 꼬이지 않도록 수집 날짜(Partition) 기반의 처리 로직 반영.

4. 최종 인프라 구성도
챗봇에게 전달할 핵심 메시지:

"나는 네이버 웹툰의 주간 차트 히스토리를 관리하려고 해. 비용은 GCP Always Free 범위 내에서 해결하고 싶고, 나중에 분석 항목이 늘어날 것에 대비해서 **'HTML 원본 저장(GCS) + 정제 데이터 적재(BigQuery)'**라는 분리된 구조를 원해. 버전 관리와 배포는 GitHub Actions로 자동화하는 하이브리드 방식을 채택했어. 이 요구사항에 맞춰서 전체 파이프라인 코드를 짜줘."